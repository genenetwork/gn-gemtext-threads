# Speeding up GEMMA

GEMMA is slow, but usually fast enough. Earlier I wrote gemma-wrapper to speed things up. In genenetwork.org, by using gemma-wrapper with LOCO, most traits are mapped in a few seconds on a a large server (30 individuals x 200K markers). By expanding makers to over 1 million, however, runtimes degrade to 6 minutes. Increasing the number of individuals to 1000 may slow mapping down to hour(s). As we are running 'precompute' on 13K traits - and soon maybe millions - it would be beneficial to reduce runtimes again.

One thing to look at is Sen's bulklmm. It can do phenotypes in parallel, provided there is no missing data. This is perfect for permutations which we'll also do. For multiple phenotypes it is a bit tricky however, because you'll have to mix and match experiments to show the same individuals (read samples).

So the approach is to first analyze steps in GEMMA and see where it is particularly inefficient. Maybe we can do something about that. I note I started the pangemma effort (and mgamma effort before). The idea is to use a propagator network for incremental improvements and also to introduce a new build system and testing framework. In parallel we'll try to scale out on HPC using Arun's ravanan software.

There is no such thing as a free lunch. So, let's dive in.

# Description

# Tags

* assigned: pjotrp
* type: feature
* priority: high

# Tasks

* [X] Try gzipped version
* [X] Run without debug
* [ ] Use lmdb for genotypes
* -   [ ] convert genotypes to lmdb
* -   [ ] replace GEMMA ReadGenotypes
* -   [ ] replace reading genotypes in AnalyzeBimbam
* [ ] Use lmdb for SNPs?
* [ ] Optimize openblas for target architecture
* [ ] Try a faster malloc library for GEMMA
* [ ] Other improvements...

# Analysis

As a test case we'll take on of the runs:

```
time -v /bin/gemma -loco 11 -k /export2/data/wrk/services/gemma-wrapper/tmp/tmp/panlmm/93f6b39ec06c09fb9ba9ca628b5fb990921b6c60.11.cXX.txt.cXX.txt -o 680029457111fdd460990f95853131c87ea20c57.11.assoc.txt -p pheno.json.txt -g pangenome-13M-genotypes.txt -a snps-matched.txt -lmm 9 -maf 0.1 -n 2 -outdir /export2/data/wrk/services/gemma-wrapper/tmp/tmp/panlmm/d20251111-588798-f81icw
```

which I simplify to

```
/bin/time -v /bin/gemma -loco 11 -k 93f6b39ec06c09fb9ba9ca628b5fb990921b6c60.11.cXX.txt.cXX.txt -p pheno.json.txt -g pangenome-13M-genotypes.txt -a snps-matched.txt -lmm 9 -maf 0.1 -n 2 -debug
Reading Files ...
number of total individuals = 143
number of analyzed individuals = 20
number of total SNPs/var        = 13209385
number of SNPS for K            = 12376792
number of SNPS for GWAS         =   832593
number of analyzed SNPs         = 13111938
```

The timer says:

```
User time (seconds): 365.33
System time (seconds): 16.59
Percent of CPU this job got: 128%
Elapsed (wall clock) time (h:mm:ss or m:ss): 4:57.01
Average shared text size (kbytes): 0
Average unshared data size (kbytes): 0
Average stack size (kbytes): 0
Average total size (kbytes): 0
Maximum resident set size (kbytes): 11073412
Average resident set size (kbytes): 0
Major (requiring I/O) page faults: 0
Minor (reclaiming a frame) page faults: 5756557
Voluntary context switches: 1365
Involuntary context switches: 478
Swaps: 0
File system inputs: 0
File system outputs: 143704
Socket messages sent: 0
Socket messages received: 0
Signals delivered: 0
Page size (bytes): 4096
Exit status: 0
```

The genotype file is unzipped at 30G. Let's try running the gzipped version (which will be beneficial on a compute cluster anyhow) which comes in at 9.2G. We know that Gemma is not the most efficient when it comes to IO. So testing is crucial.
Critically the run gets slower:

```
Percent of CPU this job got: 118%
Elapsed (wall clock) time (h:mm:ss or m:ss): 7:43.56
```

The problem is that unzip runs on a single thread in GEMMA, so it is actually slower that the gigantic raw text file.

## Running without debug

Without the debug swith gemma runs at the same speed with 128% CPU. That won't help much.

## Optimizing GEMMA+OpenBLAS+GSL

Compiling with optimization can be low hanging fruit - despite the fact that we seem to be IO bound at 128% CPU. Still, aggressive compiler optimizations may make a difference. The current build reads:

```
GEMMA Version    = 0.98.6 (2022-08-05)
Build profile    = /gnu/store/8rvid272yb53bgascf5c468z0jhsyflj-profile
GCC version      = 14.3.0
GSL Version      = 2.8
OpenBlas         = OpenBLAS 0.3.30  - OpenBLAS 0.3.30 DYNAMIC_ARCH NO_AFFINITY Cooperlake MAX_THREADS=128
arch           = Cooperlake
threads        = 96
parallel type  = threaded
```

this uses the gemma-gn2 package in

=> https://git.genenetwork.org/guix-bioinformatics/tree/gn/packages/gemma.scm#n27

which is currently not built with arch optimizations (even though Cooperlake suggests differently). Another potential optimization is to use a fast malloc library. We do, however, already compile with a recent gcc, thanks to Guix. No need to improve on that.

## Introduce lmdb for genotypes

Rather than focussing on gzip, another potential improvement is to use lmdb with mmap. We am not going to upgrade the original gemma code (which is in maintenance mode). We are going to upgrade the new pangemma project instead:

=> https://git.genenetwork.org/pangemma/

Reason being that this is our experimental project.

So I just managed to build pangemma/gemma in Guix. Next step is to introduce lmdb genotypes. Genotypes come essentially as a matrix of markers x individuals. In the case of GN geno files and BIMBAM files they are simply stored as tab delimited values and/or probabilities. This happens in

```
src/param.cpp
1261:void PARAM::ReadGenotypes(gsl_matrix *UtX, gsl_matrix *K, const bool calc_K) {
1280:void PARAM::ReadGenotypes(vector<vector<unsigned char>> &Xt, gsl_matrix *K,
```

calling into

```
gemma_io.cpp
644:bool ReadFile_geno(const string &file_geno, const set<string> &setSnps,
1752:bool ReadFile_geno(const string file_geno, vector<int> &indicator_idv,
1857:bool ReadFile_geno(const string &file_geno, vector<int> &indicator_idv,
```

which are called from gemma.cpp. Also lmm.cpp reads the geno file in the AnalyzeBimbam function (see file_geno):

```
src/lmm.cpp
61:  file_geno = cPar.file_geno;
1664:  debug_msg(file_geno);
1665:  auto infilen = file_geno.c_str();
2291:    cout << "error reading genotype file:" << file_geno << endl;
```

Note that also SNPs are read from a file (see file_snps). We already have an lmdb version for that!

So, reading genotypes happens in multiple places. In fact, it is read 1x for computing K and 2x for GWA. And it is worth than this because LOCO runs GWA 20x rereading the same files. Reading it once using lmdb should speed things up.

We'll start with the 30G 143samples.percentile.bimbam.bimbam-reduced2 file. To convert this file into lmdb we only do this once. We want to track both column and row names in the same lmdb and we will use a meta JSON record for that. On the command line we'll state wether the genotypes are stored as char or int. Floats will be packed into either of those. We'll expirement a bit to see what the default should be. A genotype is usually a number/character or a probability. In the latter case we don't have to have high precison and can choose to store an index into a range of values. We can also opt for Float16 or something more ad hoc because we don't have to store the exponent.

But let's start with a standard float here, to keep things simple. To write the first version of code I'll use a byte conversion:

```
./bin/geno2mdb.rb BXD.geno.bimbam --eval '{"0"=>0,"1"=>1,"2"=>2,"NA"=>-1}' --pack 'C*' --geno-json BXD.geno.json
```

The lmdb file contains a metadata record that looks like:

```
{
  "type": "gemma-geno",
  "version": 1,
  "eval": "G0-2",
  "key-format": "string",
  "rec-format": "C*",
  "geno": {
    "type": "gn-geno-to-gemma",
    "genofile": "BXD.geno",
    "samples": [
      "BXD1",
      "BXD2",
      "BXD5",
etc.
```

i.e. it is a self-contained, efficient, genotype format. There is also another trick, we can use Plink-style compression with

```
./bin/geno2mdb.rb BXD.geno.bimbam --eval '{"0"=>0,"1"=>1,"2"=>2,"NA"=>4}' --geno-json BXD.geno.json --gpack 'l.each_slice(4).map { |slice| slice.map.with_index.sum {|val,i| val << (i*2) } }.pack("C*")'
```

reducing the original uncompressed BIMBAM from 9.9Mb to 2.7Mb. This is still a lot larger than the gzip compressed BIMBAM, but as I pointed out earlier the uncompressed version is faster by a wide margin. Compressing the lmdb file gets it in range of the compressed BIMBAM btw. So that is always an option.

Next we create a floating point version. That reduces the file to 30% with

```
geno2mdb.rb fp.bimbam --geval 'g.to_f' --pack 'F*' --geno-json bxd_inds.list.json
```

and if we compress the probabilities into a byte reduces the file to 10%:

```
geno2mdb.rb fp.bimbam --geval '(g.to_f*255.0).to_i' --pack 'C*' --geno-json bxd_inds.list.json
```

And now the compressed version is also 4x smaller. We'll have to run gemma at scale to see what the impact is, but an uncompressed 10x reduction schould have an impact on the IO bottle neck. Note how easy it is to try these things with my little Ruby script.

=> https://github.com/genetics-statistics/gemma-wrapper/blob/master/bin/geno2mdb.rb

## Use lmdb genotypes from pangemma

Rather than writing new code in C++ I proceeded embedding guile in pangemma. If it turns out to be a performance problem we can always fall back to C. Here we show a simple test witten in guile that gets called from main.cpp:

=> https://git.genenetwork.org/pangemma/commit/?id=5b6b5e2ad97b4733125c0845cfae007e8094a687

Now I need to check:

* [ ] guile state retained between calls
* [ ] use lmdb from guile
